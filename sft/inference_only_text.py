import os
import json
import argparse
import torch
from tqdm import tqdm
from peft import PeftModel, PeftConfig
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM
from swift.utils import seed_everything
from preprocess_data import load_tables

BASE_MODEL_ID = "/data0/dywang/.cache/modelscope/hub/models/Qwen/Qwen3-4B-Instruct-2507" 

def parse_args():
    parser = argparse.ArgumentParser(description="Inference for Spider Text-to-SQL (çº¯æ–‡æœ¬æ¨¡å‹ç‰ˆ)")
    parser.add_argument("--model_type", type=str, choices=['base', 'tuned'], default='base', help="ä½¿ç”¨åŸºç¡€æ¨¡å‹(base)æˆ–å¾®è°ƒæ¨¡å‹(tuned)")
    parser.add_argument("--checkpoint_dir", type=str, default=None, help="LoRAå¾®è°ƒæƒé‡ç›®å½•")
    parser.add_argument("--output_file", type=str, required=True, help="é¢„æµ‹SQLçš„ä¿å­˜è·¯å¾„ï¼ˆå¿…å¡«ï¼‰")
    parser.add_argument("--dev_file", type=str, default="spider/dev.json", help="Spiderå¼€å‘é›†æ•°æ®è·¯å¾„")
    parser.add_argument("--tables_file", type=str, default="spider/tables.json", help="æ•°æ®åº“è¡¨ç»“æ„æ–‡ä»¶è·¯å¾„")
    return parser.parse_args()

def find_latest_checkpoint(checkpoint_dir):
    """(ä¿æŒä¸å˜) æ”¯æŒç›´æ¥æŒ‡å®šcheckpointç›®å½• + é€‚é….safetensorsæ ¼å¼LoRAæƒé‡"""
    if not os.path.exists(checkpoint_dir):
        raise FileNotFoundError(f"Checkpointç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
    
    required_files = ['adapter_config.json', 'adapter_model.safetensors']
    has_lora_files = all(os.path.exists(os.path.join(checkpoint_dir, f)) for f in required_files)
    
    if has_lora_files:
        print(f"âœ… éªŒè¯é€šè¿‡ï¼šä¼ å…¥ç›®å½•æ˜¯æœ‰æ•ˆLoRA checkpoint")
        return checkpoint_dir
    
    subdirs = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]
    checkpoints = [d for d in subdirs if 'checkpoint' in d.lower()]
    
    if not checkpoints:
        raise FileNotFoundError(f"âŒ é”™è¯¯ï¼šä¼ å…¥ç›®å½•æ— æ•ˆæˆ–æœªæ‰¾åˆ°checkpointå­ç›®å½•: {checkpoint_dir}")
    
    latest_ckpt = max(checkpoints, key=os.path.getmtime)
    print(f"âœ… ä»çˆ¶ç›®å½•æ‰¾åˆ°æœ€æ–°checkpointï¼š{latest_ckpt}")
    return latest_ckpt

def generate_predictions(args):
    seed_everything(42)
    
    print(f"=== åŸºç¡€é…ç½® (Text-Only Mode) ===")
    print(f"åŸºåº§æ¨¡å‹è·¯å¾„ï¼š{BASE_MODEL_ID}")
    print(f"æ¨¡å‹ç±»å‹ï¼š{args.model_type}")
    if args.model_type == 'tuned':
        print(f"LoRA checkpointç›®å½•ï¼š{args.checkpoint_dir}")
    print("="*50)
    
    # æ¨¡å‹åŠ è½½é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=False,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=False,
    )
    model_kwargs = {
        'device_map': 'auto',
        'dtype': torch.bfloat16,
        'quantization_config': bnb_config,
        'trust_remote_code': True,
        'low_cpu_mem_usage': True
    }

    # [ä¿®æ”¹ç‚¹ 3] åŠ è½½æ¨¡å‹é€»è¾‘ï¼šä½¿ç”¨ AutoModelForCausalLM
    print("\n=== åŠ è½½æ¨¡å‹ ===")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
    
    if args.model_type == 'base':
        # çº¯æ–‡æœ¬æ¨¡å‹ä½¿ç”¨ AutoModelForCausalLM
        model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, **model_kwargs)
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    else:
        if not args.checkpoint_dir:
            raise ValueError("âŒ ä½¿ç”¨tunedæ¨¡å¼å¿…é¡»é€šè¿‡ --checkpoint_dir æŒ‡å®šLoRAæƒé‡ç›®å½•")
            
        ckpt_path = find_latest_checkpoint(args.checkpoint_dir)
        print(f"LoRAæƒé‡è·¯å¾„ï¼š{ckpt_path}")
        
        peft_config = PeftConfig.from_pretrained(ckpt_path)
        # åŠ è½½åŸºåº§
        model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, **model_kwargs)
        # åŠ è½½LoRA
        model = PeftModel.from_pretrained(
            model,
            ckpt_path,
            device_map='auto',
            dtype=torch.bfloat16,
            trust_remote_code=True
        )
        print("âœ… å¾®è°ƒæ¨¡å‹ï¼ˆåŸºåº§+LoRAï¼‰åŠ è½½å®Œæˆ")

    # æƒé‡éªŒè¯ (ä¿æŒä¸å˜)
    print("\n=== æƒé‡åŠ è½½éªŒè¯ç»“æœ ===")
    if isinstance(model, PeftModel):
        print(f"ğŸ“Œ æ¿€æ´»çš„é€‚é…å™¨åç§°ï¼š{model.active_adapter}")
    else:
        print("ğŸ“Œ å½“å‰ä½¿ç”¨çº¯åŸºç¡€æ¨¡å‹")
    print("="*60 + "\n")

    # å‡†å¤‡æ•°æ®
    print(f"åŠ è½½æ•°æ®åº“è¡¨ç»“æ„ï¼š{args.tables_file}")
    schema_map = load_tables(args.tables_file)
    print(f"åŠ è½½å¼€å‘é›†æ•°æ®ï¼š{args.dev_file}")
    with open(args.dev_file, 'r', encoding='utf-8') as f:
        dev_data = json.load(f)
    
    predictions = []
    print("\nå¼€å§‹æ¨ç†...")
    
    for item in tqdm(dev_data, desc="ç”ŸæˆSQL"):
        db_id = item['db_id']
        question = item['question']
        
        if db_id not in schema_map:
            predictions.append("SELECT * FROM T") 
            continue
            
        schema_context = schema_map[db_id]
        
        # æ„é€  Prompt å†…å®¹
        system_content = "You are a professional SQL data analyst. " \
                         "Given a database schema and a natural language question, " \
                         "generate a valid SQL query. Do not provide any explanation, only the SQL."
        user_content = f"Database Schema:\n{schema_context}\n\nQuestion: {question}\n\nSQL:"
        
        # [ä¿®æ”¹ç‚¹ 4] ä½¿ç”¨ tokenizer.apply_chat_template æ›¿ä»£æ‰‹åŠ¨å­—ç¬¦ä¸²æ‹¼æ¥
        # è¿™æ˜¯çº¯æ–‡æœ¬å¤§æ¨¡å‹ï¼ˆQwen2/Qwen2.5/Qwen3ï¼‰çš„æ ‡å‡†ç”¨æ³•
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ]
        
        # apply_chat_template ä¼šè‡ªåŠ¨æ·»åŠ  <|im_start|>, <|im_end|> ç­‰ç‰¹æ®Š token
        text_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨æ·»åŠ  <|assistant|> æˆ–ç­‰æ•ˆçš„å¼•å¯¼ç¬¦
        )
        
        inputs = tokenizer(
            text_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096 # çº¯æ–‡æœ¬æ¨¡å‹é€šå¸¸å¯ä»¥æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´
        )
        
        input_ids = inputs['input_ids'].to(model.device)
        attention_mask = inputs['attention_mask'].to(model.device)

        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=512,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                temperature=0.01, # æ¨ç†æ—¶å¯¹äºä»£ç ç”Ÿæˆï¼Œå»ºè®®ä½¿ç”¨æä½çš„ temperature ä»¥ä¿è¯ç¡®å®šæ€§
                do_sample=False,  # ä»£ç ç”Ÿæˆé€šå¸¸ä¸éœ€è¦é‡‡æ ·ï¼Œæˆ–è€…é‡‡æ ·èŒƒå›´å¾ˆå°
            )
        
        # è§£ç 
        input_len = input_ids.shape[1]
        output_ids = generated_ids[0][input_len:]
        response = tokenizer.decode(output_ids, skip_special_tokens=True)
        
        # åå¤„ç†ï¼ˆä¿æŒä¸å˜ï¼‰
        cleaned_sql = response.strip()
        if "```sql" in cleaned_sql:
            cleaned_sql = cleaned_sql.split("```sql")[1].split("```")[0].strip()
        elif "```" in cleaned_sql:
            cleaned_sql = cleaned_sql.split("```")[0].strip()
        cleaned_sql = cleaned_sql.replace('\n', ' ')
        
        predictions.append(cleaned_sql)
    
    print(f"\næ¨ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(predictions)} æ¡SQLè¯­å¥")
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    with open(args.output_file, 'w', encoding='utf-8') as f:
        for sql in predictions:
            f.write(sql + '\n')
    print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°ï¼š{args.output_file}")

if __name__ == "__main__":
    args = parse_args()
    generate_predictions(args)