#!/usr/bin/env python3
"""
åŒè¯­æ•°æ®å¤„ç†ä¸“å®¶ - ä¸»ç¨‹åºå…¥å£
Main Entry for Bilingual Data Processing Expert

ä½¿ç”¨æ–¹æ³•:
    # æµ‹è¯•APIè¿æ¥
    python main.py test
    
    # è¿è¡Œå®Œæ•´Pipeline
    python main.py run --data path/to/data.json --schema path/to/tables.json
    
    # ä»…ç¿»è¯‘
    python main.py translate --data path/to/data.json --n 3
    
    # ä»…åˆæˆ
    python main.py synthesize --domains ä¼ä¸šé”€å”® å­¦ç”Ÿæˆç»© --n 20
    
    # ç”Ÿæˆè¯„æµ‹æ•°æ®
    python main.py eval --data path/to/train.json --schema path/to/tables.json
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from data_expert import (
    QwenClient,
    QuotaExhaustedError,
    APIServiceError,
    DataCleaner,
    BilingualTranslator,
    DataSynthesizer,
    EvalDataGenerator,
    DataExpertPipeline
)
from data_expert.api_client import test_connection

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def cmd_test(args):
    """æµ‹è¯•APIè¿æ¥"""
    print("=" * 50)
    print("æµ‹è¯•é€šä¹‰åƒé—®APIè¿æ¥...")
    print("=" * 50)
    
    success = test_connection()
    
    if success:
        print("\nâœ… APIè¿æ¥æµ‹è¯•æˆåŠŸï¼å¯ä»¥å¼€å§‹ä½¿ç”¨æ•°æ®å¤„ç†ä¸“å®¶ã€‚")
    else:
        print("\nâŒ APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š")
        print("1. æ˜¯å¦è®¾ç½®äº†ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        print("2. API Keyæ˜¯å¦æœ‰æ•ˆ")
        print("3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("\nè®¾ç½®æ–¹æ³•: export DASHSCOPE_API_KEY='sk-your-key-here'")
    
    return success


def cmd_translate(args):
    """ç¿»è¯‘å‘½ä»¤"""
    print("=" * 50)
    print("å¼€å§‹åŒè¯­ç¿»è¯‘...")
    print("=" * 50)
    
    pipeline = DataExpertPipeline(output_dir=args.output)
    
    # åŠ è½½æ•°æ®
    samples = pipeline.load_spider_data(args.data)
    schema_dict = pipeline.load_schema(args.schema) if args.schema else {}
    
    # é™åˆ¶æ•°é‡
    if args.limit:
        samples = samples[:args.limit]
    
    # ç¿»è¯‘
    result = pipeline.run_translation_pipeline(
        samples, schema_dict,
        n_variants=args.n,
        include_dirty=args.dirty
    )
    
    print(f"\nâœ… ç¿»è¯‘å®Œæˆï¼ç”Ÿæˆ {len(result)} æ¡æ•°æ®")
    print(f"ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
    
    return result


def cmd_synthesize(args):
    """åˆæˆå‘½ä»¤"""
    print("=" * 50)
    print("å¼€å§‹æ•°æ®åˆæˆ...")
    print("=" * 50)
    
    pipeline = DataExpertPipeline(output_dir=args.output)
    
    # æ˜¾ç¤ºå¯ç”¨é¢†åŸŸ
    available = pipeline.synthesizer.get_available_domains()
    print(f"å¯ç”¨é¢†åŸŸ: {available}")
    
    # ç¡®å®šè¦ä½¿ç”¨çš„é¢†åŸŸ
    domains = args.domains if args.domains else available
    
    # åˆæˆ
    result = pipeline.run_synthesis_pipeline(
        domains=domains,
        n_per_domain=args.n
    )
    
    print(f"\nâœ… åˆæˆå®Œæˆï¼ç”Ÿæˆ {len(result)} æ¡æ•°æ®")
    print(f"ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
    
    return result


def cmd_clean(args):
    """æ¸…æ´—å‘½ä»¤"""
    print("=" * 50)
    print("å¼€å§‹æ•°æ®æ¸…æ´—...")
    print("=" * 50)
    
    pipeline = DataExpertPipeline(output_dir=args.output)
    
    # åŠ è½½æ•°æ®
    samples = pipeline.load_spider_data(args.data)
    schema_dict = pipeline.load_schema(args.schema) if args.schema else {}
    
    # é™åˆ¶æ•°é‡
    if args.limit:
        samples = samples[:args.limit]
    
    # æ¸…æ´—
    result = pipeline.run_cleaning_pipeline(
        samples, schema_dict,
        confidence_threshold=args.threshold
    )
    
    print(f"\nâœ… æ¸…æ´—å®Œæˆï¼")
    print(f"  æœ‰æ•ˆæ ·æœ¬: {len(result['valid'])}")
    print(f"  æ— æ•ˆæ ·æœ¬: {len(result['invalid'])}")
    print(f"ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
    
    return result


def cmd_eval(args):
    """ç”Ÿæˆè¯„æµ‹æ•°æ®å‘½ä»¤"""
    print("=" * 50)
    print("å¼€å§‹ç”Ÿæˆè¯„æµ‹æ•°æ®...")
    print("=" * 50)
    
    pipeline = DataExpertPipeline(output_dir=args.output)
    
    # åŠ è½½æ•°æ®
    samples = pipeline.load_spider_data(args.data)
    schema_dict = pipeline.load_schema(args.schema) if args.schema else {}
    
    # é™åˆ¶æ•°é‡
    if args.limit:
        samples = samples[:args.limit]
    
    # ç”Ÿæˆè¯„æµ‹æ•°æ®
    result = pipeline.run_eval_generation_pipeline(
        samples, schema_dict,
        holdout_ratio=args.ratio
    )
    
    total = sum(len(v) for k, v in result.items() if isinstance(v, list))
    print(f"\nâœ… è¯„æµ‹æ•°æ®ç”Ÿæˆå®Œæˆï¼å…± {total} æ¡")
    print(f"ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
    
    return result


def cmd_run(args):
    """è¿è¡Œå®Œæ•´Pipeline"""
    print("=" * 50)
    print("è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†Pipeline...")
    print("=" * 50)
    
    pipeline = DataExpertPipeline(output_dir=args.output)
    
    try:
        result = pipeline.run_full_pipeline(
            source_data_path=args.data,
            schema_path=args.schema,
            clean=not args.no_clean,
            translate=not args.no_translate,
            synthesize=not args.no_synthesize,
            generate_eval=not args.no_eval,
            synthesis_domains=args.domains,
            n_translation_variants=args.n_translate,
            n_synthesis_per_domain=args.n_synthesize
        )
        
        if result.get("status") == "completed":
            print("\nâœ… å®Œæ•´Pipelineæ‰§è¡Œå®Œæˆï¼")
        elif result.get("status") == "quota_exhausted":
            print("\nâš ï¸ APIé…é¢å·²è€—å°½ï¼ŒPipelineå·²ä¸­æ–­")
            print("å·²å¤„ç†çš„æ•°æ®å·²ä¿å­˜ï¼Œè¯·æŸ¥çœ‹ checkpoints ç›®å½•")
            print("\næ¢å¤æ‰§è¡Œå‘½ä»¤:")
            print(f"  python main.py resume --checkpoint <checkpoint_file>")
        else:
            print(f"\nâŒ Pipelineæ‰§è¡Œå‡ºé”™: {result.get('status')}")
            
        print(f"ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
        print("\næ‰§è¡Œæ‘˜è¦:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
        return result
        
    except QuotaExhaustedError as e:
        print(f"\nâŒ APIé…é¢è€—å°½: {e}")
        print("å·²å¤„ç†çš„æ•°æ®å·²è‡ªåŠ¨ä¿å­˜åˆ° checkpoints ç›®å½•")
        print("\nè¯·æŸ¥çœ‹é”™è¯¯æŠ¥å‘Šè·å–æ¢å¤å‘½ä»¤:")
        print(f"  ls {pipeline.checkpoint_dir}/error_report_*.json")
        return None


def cmd_resume(args):
    """ä»æ–­ç‚¹æ¢å¤æ‰§è¡Œ"""
    print("=" * 50)
    print("ä»æ–­ç‚¹æ¢å¤æ‰§è¡Œ...")
    print("=" * 50)
    
    pipeline = DataExpertPipeline(output_dir=args.output)
    
    try:
        result = pipeline.resume_from_checkpoint(args.checkpoint)
        
        if result.get("status") == "completed":
            print(f"\nâœ… æ¢å¤æ‰§è¡Œå®Œæˆï¼")
            print(f"  - æ€»è®¡å¤„ç†: {result.get('total_count')} æ¡")
            print(f"  - ä»æ–­ç‚¹æ¢å¤: {result.get('resumed_from_count')} æ¡å·²å­˜åœ¨")
        else:
            print(f"\nâš ï¸ æ¢å¤æ‰§è¡ŒçŠ¶æ€: {result.get('status')}")
        
        print(f"ç»“æœä¿å­˜åœ¨: {pipeline.output_dir}")
        return result
        
    except Exception as e:
        print(f"\nâŒ æ¢å¤æ‰§è¡Œå¤±è´¥: {e}")
        return None


def cmd_list_checkpoints(args):
    """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹"""
    print("=" * 50)
    print("å¯ç”¨çš„æ£€æŸ¥ç‚¹:")
    print("=" * 50)
    
    from data_expert.config import CHECKPOINT_DIR
    checkpoint_dir = Path(CHECKPOINT_DIR)
    
    if not checkpoint_dir.exists():
        print("æš‚æ— æ£€æŸ¥ç‚¹")
        return
    
    checkpoints = sorted(checkpoint_dir.glob("checkpoint_*.json"), reverse=True)
    
    if not checkpoints:
        print("æš‚æ— æ£€æŸ¥ç‚¹")
        return
    
    for cp in checkpoints:
        with open(cp, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"\nğŸ“ {cp.name}")
        print(f"   æ—¶é—´: {data.get('timestamp')}")
        print(f"   é˜¶æ®µ: {data.get('stage')}")
        print(f"   å·²å¤„ç†: {data.get('processed_count')} æ¡")
        print(f"   å‰©ä½™: {data.get('remaining_count')} æ¡")
    
    # åˆ—å‡ºé”™è¯¯æŠ¥å‘Š
    errors = sorted(checkpoint_dir.glob("error_report_*.json"), reverse=True)
    if errors:
        print("\n" + "=" * 50)
        print("é”™è¯¯æŠ¥å‘Š:")
        print("=" * 50)
        for err in errors[:5]:  # åªæ˜¾ç¤ºæœ€è¿‘5ä¸ª
            with open(err, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"\nâš ï¸ {err.name}")
            print(f"   æ—¶é—´: {data.get('timestamp')}")
            print(f"   é˜¶æ®µ: {data.get('stage')}")
            print(f"   é”™è¯¯ç±»å‹: {data.get('error_type')}")
            print(f"   è¿›åº¦: {data.get('progress_percentage')}")


def cmd_demo(args):
    """è¿è¡Œæ¼”ç¤º"""
    print("=" * 50)
    print("è¿è¡Œæ•°æ®å¤„ç†ä¸“å®¶æ¼”ç¤º...")
    print("=" * 50)
    
    # æµ‹è¯•è¿æ¥
    if not test_connection():
        return
    
    client = QwenClient()
    
    # æ¼”ç¤ºç¿»è¯‘
    print("\n--- 1. ç¿»è¯‘æ¼”ç¤º ---")
    translator = BilingualTranslator(client)
    
    sample = {
        "question": "Find the names of students who scored above 90",
        "sql": "SELECT name FROM students WHERE score > 90",
        "db_id": "school"
    }
    
    schema = {
        "tables": [{"name": "students", "columns": ["id", "name", "score"]}]
    }
    
    translated = translator.translate_sample(
        question_en=sample["question"],
        sql=sample["sql"],
        schema=schema,
        db_id=sample["db_id"],
        n_variants=2
    )
    
    print("ç¿»è¯‘ç»“æœ:")
    for t in translated:
        print(f"  ä¸­æ–‡: {t.get('question_zh', 'N/A')}")
        print(f"  SQL: {t.get('sql', 'N/A')}")
        print()
    
    # æ¼”ç¤ºåˆæˆ
    print("\n--- 2. æ•°æ®åˆæˆæ¼”ç¤º ---")
    synthesizer = DataSynthesizer(client)
    print(f"å¯ç”¨é¢†åŸŸ: {synthesizer.get_available_domains()}")
    
    synthesized = synthesizer.synthesize_from_domain("å­¦ç”Ÿæˆç»©", n_samples=3)
    print(f"ç”Ÿæˆäº† {len(synthesized)} æ¡åˆæˆæ•°æ®")
    for s in synthesized[:2]:
        print(f"  é—®é¢˜: {s.get('question_zh', 'N/A')}")
        print(f"  SQL: {s.get('sql', 'N/A')}")
        print()
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nä½¿ç”¨å®Œæ•´Pipelineçš„ç¤ºä¾‹å‘½ä»¤:")
    print("  python main.py run --data your_data.json --schema tables.json")


def main():
    parser = argparse.ArgumentParser(
        description="åŒè¯­æ•°æ®å¤„ç†ä¸“å®¶ - Text-to-SQLæ•°æ®å¤„ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æµ‹è¯•APIè¿æ¥
  python main.py test
  
  # è¿è¡Œæ¼”ç¤º
  python main.py demo
  
  # ç¿»è¯‘æ•°æ®
  python main.py translate --data spider_train.json --n 3
  
  # åˆæˆæ•°æ®
  python main.py synthesize --domains ä¼ä¸šé”€å”® å­¦ç”Ÿæˆç»© --n 20
  
  # è¿è¡Œå®Œæ•´Pipeline
  python main.py run --data spider_train.json --schema tables.json
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # testå‘½ä»¤
    test_parser = subparsers.add_parser("test", help="æµ‹è¯•APIè¿æ¥")
    
    # demoå‘½ä»¤
    demo_parser = subparsers.add_parser("demo", help="è¿è¡Œæ¼”ç¤º")
    
    # translateå‘½ä»¤
    translate_parser = subparsers.add_parser("translate", help="ç¿»è¯‘æ•°æ®")
    translate_parser.add_argument("--data", required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„")
    translate_parser.add_argument("--schema", help="Schemaæ–‡ä»¶è·¯å¾„")
    translate_parser.add_argument("--output", default="./generated_data", help="è¾“å‡ºç›®å½•")
    translate_parser.add_argument("--n", type=int, default=3, help="æ¯ä¸ªæ ·æœ¬çš„å˜ä½“æ•°é‡")
    translate_parser.add_argument("--limit", type=int, help="å¤„ç†çš„æ ·æœ¬æ•°é‡é™åˆ¶")
    translate_parser.add_argument("--dirty", action="store_true", help="åŒ…å«è„æ•°æ®å˜ä½“")
    
    # synthesizeå‘½ä»¤
    synth_parser = subparsers.add_parser("synthesize", help="åˆæˆæ•°æ®")
    synth_parser.add_argument("--domains", nargs="+", help="é¢†åŸŸåˆ—è¡¨")
    synth_parser.add_argument("--output", default="./generated_data", help="è¾“å‡ºç›®å½•")
    synth_parser.add_argument("--n", type=int, default=20, help="æ¯ä¸ªé¢†åŸŸçš„æ ·æœ¬æ•°é‡")
    
    # cleanå‘½ä»¤
    clean_parser = subparsers.add_parser("clean", help="æ¸…æ´—æ•°æ®")
    clean_parser.add_argument("--data", required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„")
    clean_parser.add_argument("--schema", help="Schemaæ–‡ä»¶è·¯å¾„")
    clean_parser.add_argument("--output", default="./generated_data", help="è¾“å‡ºç›®å½•")
    clean_parser.add_argument("--threshold", type=float, default=0.8, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    clean_parser.add_argument("--limit", type=int, help="å¤„ç†çš„æ ·æœ¬æ•°é‡é™åˆ¶")
    
    # evalå‘½ä»¤
    eval_parser = subparsers.add_parser("eval", help="ç”Ÿæˆè¯„æµ‹æ•°æ®")
    eval_parser.add_argument("--data", required=True, help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    eval_parser.add_argument("--schema", help="Schemaæ–‡ä»¶è·¯å¾„")
    eval_parser.add_argument("--output", default="./generated_data", help="è¾“å‡ºç›®å½•")
    eval_parser.add_argument("--ratio", type=float, default=0.1, help="holdoutæ¯”ä¾‹")
    eval_parser.add_argument("--limit", type=int, help="å¤„ç†çš„æ ·æœ¬æ•°é‡é™åˆ¶")
    
    # runå‘½ä»¤
    run_parser = subparsers.add_parser("run", help="è¿è¡Œå®Œæ•´Pipeline")
    run_parser.add_argument("--data", required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„")
    run_parser.add_argument("--schema", help="Schemaæ–‡ä»¶è·¯å¾„")
    run_parser.add_argument("--output", default="./generated_data", help="è¾“å‡ºç›®å½•")
    run_parser.add_argument("--domains", nargs="+", help="åˆæˆæ•°æ®çš„é¢†åŸŸåˆ—è¡¨")
    run_parser.add_argument("--n-translate", type=int, default=3, help="ç¿»è¯‘å˜ä½“æ•°é‡")
    run_parser.add_argument("--n-synthesize", type=int, default=20, help="æ¯é¢†åŸŸåˆæˆæ•°é‡")
    run_parser.add_argument("--no-clean", action="store_true", help="è·³è¿‡æ¸…æ´—æ­¥éª¤")
    run_parser.add_argument("--no-translate", action="store_true", help="è·³è¿‡ç¿»è¯‘æ­¥éª¤")
    run_parser.add_argument("--no-synthesize", action="store_true", help="è·³è¿‡åˆæˆæ­¥éª¤")
    run_parser.add_argument("--no-eval", action="store_true", help="è·³è¿‡è¯„æµ‹æ•°æ®ç”Ÿæˆ")
    
    # resumeå‘½ä»¤ - ä»æ–­ç‚¹æ¢å¤
    resume_parser = subparsers.add_parser("resume", help="ä»æ–­ç‚¹æ¢å¤æ‰§è¡Œ")
    resume_parser.add_argument("--checkpoint", required=True, help="æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„")
    resume_parser.add_argument("--output", default="./generated_data", help="è¾“å‡ºç›®å½•")
    
    # checkpointså‘½ä»¤ - åˆ—å‡ºæ£€æŸ¥ç‚¹
    checkpoints_parser = subparsers.add_parser("checkpoints", help="åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹å’Œé”™è¯¯æŠ¥å‘Š")
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    # æ‰§è¡Œå¯¹åº”å‘½ä»¤
    commands = {
        "test": cmd_test,
        "demo": cmd_demo,
        "translate": cmd_translate,
        "synthesize": cmd_synthesize,
        "clean": cmd_clean,
        "eval": cmd_eval,
        "run": cmd_run,
        "resume": cmd_resume,
        "checkpoints": cmd_list_checkpoints
    }
    
    cmd_func = commands.get(args.command)
    if cmd_func:
        cmd_func(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
